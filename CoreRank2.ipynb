{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import sys\n",
    "import os\n",
    "from io import open\n",
    "from collections import defaultdict, Counter\n",
    "import networkx as nx\n",
    "import regex as re\n",
    "import pickle\n",
    "import json\n",
    "import pkuseg\n",
    "from tqdm import tqdm\n",
    "import emoji\n",
    "import math\n",
    "\n",
    "\n",
    "# %%\n",
    "def ngrams(lst, n=4):\n",
    "    N = len(lst)\n",
    "    if N < n:\n",
    "        return\n",
    "        yield\n",
    "\n",
    "    for i in range(N-n+1):\n",
    "        yield lst[i: i+n]\n",
    "\n",
    "\n",
    "def removeSelfLoops(G):\n",
    "    '''删除自环边'''\n",
    "    nodes_with_selfloops = G.nodes_with_selfloops()\n",
    "    for node in nodes_with_selfloops:\n",
    "        G.remove_edge(node, node)\n",
    "    return G\n",
    "\n",
    "\n",
    "def removeSingletons(G):\n",
    "    '''删除没有边单独的节点'''\n",
    "    degrees = dict(G.degree())\n",
    "    for node in degrees.keys():\n",
    "        if degrees[node] == 0:\n",
    "            G.remove_node(node)\n",
    "    return G\n",
    "\n",
    "\n",
    "def getCoreNumbers(Gpassed):\n",
    "    '''计算 k-core number'''\n",
    "    G = Gpassed.copy()\n",
    "    G = removeSelfLoops(G)\n",
    "    G = removeSingletons(G)\n",
    "\n",
    "    degrees = dict(G.degree())\n",
    "    dmax = max(degrees.values()) + 2\n",
    "    corenumbers = {}\n",
    "\n",
    "    N = len(degrees.values())\n",
    "    for corenumber_k in range(2, dmax):\n",
    "        whilec = 0\n",
    "        while min(degrees.values()) < corenumber_k:\n",
    "            whilec += 1\n",
    "            for node in degrees.keys():  # remove all the nodes with degrees < k\n",
    "                if degrees[node] < corenumber_k:\n",
    "                    corenumbers[node] = corenumber_k - 1\n",
    "                    G.remove_node(node)\n",
    "\n",
    "            degrees = dict(G.degree())\n",
    "            if len(degrees.values()) == 0:\n",
    "                break\n",
    "\n",
    "        if len(degrees.values()) == 0:\n",
    "            break\n",
    "\n",
    "    return corenumbers\n",
    "\n",
    "\n",
    "def getCoreRank(G, core_number):\n",
    "    '''The CoreRank number of a node is defined as \n",
    "    the sum of the core numbers of its neighbors. '''\n",
    "    core_ranks = {}\n",
    "    for node in list(G.keys()):\n",
    "        core_ranks[str(node)] = 0\n",
    "        for nbr in list(G[node].keys()):\n",
    "            core_ranks[str(node)] += core_number[nbr]\n",
    "    core_ranks = dict(sorted(core_ranks.items(), key=lambda x: x[1], reverse=True))\n",
    "    return core_ranks\n",
    "\n",
    "\n",
    "def inverse_document_frequencies(wordlist, documents):\n",
    "    idf_values = {}\n",
    "    for tkn in tqdm(wordlist):\n",
    "        contains_token = map(lambda doc: tkn in doc, documents)  # 01 list shows the token is in each sentence or not\n",
    "#         if int(sum(contains_token)) == 0:\n",
    "#             print(tkn)  # emoji name\n",
    "        idf_values[tkn] = 1 + math.log(len(documents)/float(sum(contains_token) + 1))\n",
    "    return idf_values\n",
    "\n",
    "\n",
    "def tw_idf(core_rank_score, idf):\n",
    "    twidf = {}\n",
    "    for w in idf.keys():\n",
    "        twidf[w] = core_rank_score[w] * idf[w]\n",
    "    twidf = dict(sorted(twidf.items(), key=lambda x: x[1], reverse=True))\n",
    "    return twidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 151337/151337 [03:14<00:00, 777.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "with open(\".//data//neike_all.json\", \"r\", encoding='utf-8') as f:\n",
    "    dialogs = json.load(f)\n",
    "\n",
    "useless = [\"谢谢\", \"感谢\", \"感激\", \"不客气\", \"早日康复\", \"祝你\", \"满意\"]\n",
    "pucs = '''’'#＜&〰＇『％～—（「\\.｠–~？\\[|〚｟〟〛』）>_〙„／\\$｜＋\\/\\^、；｣\"\\*\\\\,〘“〔〿@〃＂】：｀‘＠:\\]\\)\\}＞【＾〞〖!\\{;！」‛\\(－=＄‟［”＃〜﹏＆｢〾…＼〕｡`｝＝\\+］《＿＊〗\\?〝，<､》%。｛‧'''\n",
    "re_punctuation = \"[{}]+\".format(pucs)\n",
    "\n",
    "fw = open(\".//data//neike_sentences.txt\", \"w\", encoding='utf-8')\n",
    "\n",
    "dlg_set = []\n",
    "for dlg in tqdm(dialogs.values()):\n",
    "    if dlg not in dlg_set:\n",
    "        dlg_set.append(dlg)\n",
    "        for sen in dlg:\n",
    "            sen = sen.replace(\"患者:\", \"\")\n",
    "            sen = sen.replace(\"医生:\", \"\")\n",
    "            sen = re.sub(re_punctuation, \" \", sen)  # 标点符号替换成空格\n",
    "            if(len(sen) > 5):  # 过滤过短句子\n",
    "                cnt = 0\n",
    "                for w in useless:\n",
    "                    if w in sen:\n",
    "                        cnt = 1   # 过滤无用句子\n",
    "                if(cnt == 0):\n",
    "                    fw.write(sen + \"\\n\")\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 1846544/1846544 [14:50<00:00, 2072.67it/s]\n",
      "100%|██████████████████████████████████████| 125978/125978 [8:49:31<00:00,  3.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "window_size = 4\n",
    "\n",
    "input_edges_fname = f\".//data//neike_all_input_edges_win{window_size}.txt\"\n",
    "fw = open(input_edges_fname, 'w', encoding='utf-8')\n",
    "\n",
    "with open('.//data//medical_dict.json', 'r', encoding='utf-8') as f:\n",
    "    user_dict = json.load(f)\n",
    "seg = pkuseg.pkuseg(user_dict=user_dict)\n",
    "\n",
    "with open('.//data//stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = f.readlines()\n",
    "stopwords = set([w.replace(\"\\n\", \"\") for w in stopwords])\n",
    "\n",
    "with open(\".//data//neike_sentences.txt\", 'r', encoding='utf-8') as f:\n",
    "    sens = f.readlines()\n",
    "    sens = [sen.replace(\"\\n\", \"\") for sen in sens]\n",
    "\n",
    "for sen in tqdm(sens):\n",
    "    sen = emoji.demojize(string=sen)\n",
    "    tokens = seg.cut(sen)\n",
    "    tokens = [w for w in tokens if w not in stopwords]\n",
    "    if(len(tokens) < 5):\n",
    "        continue\n",
    "    for ngram in ngrams(tokens, n=window_size):\n",
    "        for nbr in ngram[1:]:\n",
    "            fw.write(ngram[0] + ' ' + nbr + '\\n')\n",
    "\n",
    "fw.close()\n",
    "\n",
    "\n",
    "graphpath = input_edges_fname\n",
    "G = nx.read_edgelist(graphpath, delimiter=' ', nodetype=str)\n",
    "gadj = G.adj\n",
    "\n",
    "core_number = getCoreNumbers(G)\n",
    "\n",
    "core_rank_score = getCoreRank(G.adj, core_number)\n",
    "\n",
    "with open(\".//data//neike_core_rank_socres.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(fp=f, obj=core_rank_score, indent=2, ensure_ascii=False)\n",
    "\n",
    "wordlist = list(core_number.keys())\n",
    "idf = inverse_document_frequencies(wordlist, sens)\n",
    "\n",
    "twidf = tw_idf(core_rank_score, idf)\n",
    "\n",
    "with open(\".//data//neike_twidf.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(fp=f, obj=twidf, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
